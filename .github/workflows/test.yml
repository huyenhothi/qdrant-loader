name: Test and Coverage

on:
  push:
    branches: [ main, develop, feature/*, bugfix/*, release/*, fix/* ]
  pull_request:
    branches: [ main, develop, feature/*, bugfix/*, release/*, fix/* ]

permissions:
  contents: read
  actions: read

concurrency:
  group: "test-${{ github.ref }}"
  cancel-in-progress: true

jobs:
  test-core:
    name: Test Core Package
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install core with extras used in tests
          pip install -e packages/qdrant-loader-core[openai,ollama]
          # Install repo dev deps for pytest, coverage, etc.
          pip install -e .[dev]

      - name: Run core tests and generate coverage reports
        run: |
          cd packages/qdrant-loader-core
          python -m pytest tests/ --cov=src --cov-report=xml:../../coverage-core.xml --cov-report=html:../../htmlcov-core -v

      # - name: Upload core coverage artifact
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: coverage-core-${{ github.run_id }}
      #     path: |
      #       htmlcov-core
      #       coverage-core.xml
      #     retention-days: 30
      
  test-loader:
    name: Test QDrant Loader
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install system dependencies
        run: |
          # Install ffmpeg for full MarkItDown audio processing capabilities
          # This ensures comprehensive file conversion testing including audio files
          sudo apt-get update
          sudo apt-get install -y ffmpeg

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install monorepo packages locally in dependency order to avoid PyPI resolution
          pip install -e packages/qdrant-loader-core[openai]
          pip install -e .[dev]
          pip install -e packages/qdrant-loader

      - name: Run loader tests and generate coverage reports
        run: |
          cd packages/qdrant-loader
          python -m pytest tests/unit -n 2 --cov=src -v
          python -m pytest tests/integration --cov=src -v

      # - name: Upload loader coverage artifact
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: coverage-loader-${{ github.run_id }}
      #     path: |
      #       htmlcov-loader
      #       coverage-loader.xml
      #     retention-days: 30

  test-mcp-server:
    name: Test MCP Server
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install monorepo packages locally in dependency order to avoid PyPI resolution
          pip install -e packages/qdrant-loader-core
          pip install -e .[dev]
          pip install -e packages/qdrant-loader
          pip install -e packages/qdrant-loader-mcp-server

      - name: Create .env.test file for MCP server
        run: |
          cd packages/qdrant-loader-mcp-server
          cp tests/.env.test.template tests/.env.test
          
          # Check if required secrets are set
          if [ -z "${{ secrets.QDRANT_URL }}" ]; then
            echo "Error: QDRANT_URL secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.QDRANT_API_KEY }}" ]; then
            echo "Error: QDRANT_API_KEY secret is not set"
            exit 1
          fi
          if [ -z "${{ secrets.QDRANT_COLLECTION_NAME }}" ]; then
            echo "Error: QDRANT_COLLECTION_NAME secret is not set"
            exit 1
          fi
          
          # Replace environment variables with secrets for integration tests
          sed -i "s|QDRANT_URL=.*|QDRANT_URL=${{ secrets.QDRANT_URL }}|g" tests/.env.test
          sed -i "s|QDRANT_API_KEY=.*|QDRANT_API_KEY=${{ secrets.QDRANT_API_KEY }}|g" tests/.env.test
          sed -i "s|QDRANT_COLLECTION_NAME=.*|QDRANT_COLLECTION_NAME=${{ secrets.QDRANT_COLLECTION_NAME }}|g" tests/.env.test
          
          echo "Created .env.test file for MCP server successfully"
          echo "Contents (with secrets masked):"
          sed 's/=.*/=***/' tests/.env.test

      - name: Run MCP server tests and generate coverage reports
        run: |
          cd packages/qdrant-loader-mcp-server
          python -m pytest -n 2 tests/unit --cov=src -v
          python -m pytest tests/integration --cov=src -v

      # - name: Upload MCP server coverage artifact
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: coverage-mcp-${{ github.run_id }}
      #     path: |
      #       htmlcov-mcp
      #       coverage-mcp.xml
      #     retention-days: 30

  test-website:
    name: Test Website Build System
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install system dependencies for website testing
        run: |
          # Install system dependencies that might be needed for favicon generation
          sudo apt-get update
          sudo apt-get install -y libcairo2-dev libgirepository1.0-dev

      - name: Install website test dependencies
        run: |
          python -m pip install --upgrade pip
          # Install dev dependencies from root pyproject.toml which includes requests, responses, and other test dependencies
          pip install -e .[dev]
          # Install optional docs dependencies for comprehensive testing
          pip install -e .[docs] || echo "Optional docs dependencies not available"

      - name: Run website tests with coverage
        run: |
          # Add website directory to Python path and run tests
          export PYTHONPATH="${PYTHONPATH}:$(pwd)/website"
          python -m pytest tests/ --cov=website --cov-report=xml:coverage-website.xml --cov-report=html:htmlcov-website --cov-report=term-missing -v

      # - name: Upload website test coverage artifact
      #   uses: actions/upload-artifact@v4
      #   if: always()
      #   with:
      #     name: coverage-website-${{ github.run_id }}
      #     path: |
      #       htmlcov-website
      #       coverage-website.xml
      #     retention-days: 30

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-core, test-loader, test-mcp-server, test-website]
    if: always()
    steps:
      - name: Check test results
        run: |
          echo "=== Test Results Summary ==="
          echo "Core Package Tests: ${{ needs.test-core.result }}"
          echo "QDrant Loader Tests: ${{ needs.test-loader.result }}"
          echo "MCP Server Tests: ${{ needs.test-mcp-server.result }}"
          echo "Website Build Tests: ${{ needs.test-website.result }}"
          
          if [ "${{ needs.test-core.result }}" != "success" ] || [ "${{ needs.test-loader.result }}" != "success" ] || [ "${{ needs.test-mcp-server.result }}" != "success" ] || [ "${{ needs.test-website.result }}" != "success" ]; then
            echo "âŒ Some tests failed"
            exit 1
          else
            echo "âœ… All tests passed"
          fi

      - name: Create test status artifact
        run: |
          mkdir -p test-results
          echo "{\n            \"core_status\": \"${{ needs.test-core.result }}\",\n            \"loader_status\": \"${{ needs.test-loader.result }}\",\n            \"mcp_status\": \"${{ needs.test-mcp-server.result }}\",\n            \"website_status\": \"${{ needs.test-website.result }}\",\n            \"overall_status\": \"${{ (needs.test-core.result == 'success' && needs.test-loader.result == 'success' && needs.test-mcp-server.result == 'success' && needs.test-website.result == 'success') && 'success' || 'failure' }}\",\n            \"run_id\": \"${{ github.run_id }}\",\n            \"commit_sha\": \"${{ github.sha }}\",\n            \"branch\": \"${{ github.ref_name }}\",\n            \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n          }" > test-results/status.json
          
          echo "Test run completed at $(date)" > test-results/summary.txt
          echo "Run ID: ${{ github.run_id }}" >> test-results/summary.txt
          echo "Commit: ${{ github.sha }}" >> test-results/summary.txt
          echo "Branch: ${{ github.ref_name }}" >> test-results/summary.txt
          echo "Core Package Tests: ${{ needs.test-core.result }}" >> test-results/summary.txt
          echo "Loader Tests: ${{ needs.test-loader.result }}" >> test-results/summary.txt
          echo "MCP Server Tests: ${{ needs.test-mcp-server.result }}" >> test-results/summary.txt
          echo "Website Tests: ${{ needs.test-website.result }}" >> test-results/summary.txt

      # - name: Upload test status artifact
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: test-status-${{ github.run_id }}
      #     path: test-results/
      #     retention-days: 30
